from urllib import request
from bs4 import BeautifulSoup
import re
import os
import urllib

# connect to website and get list of all pdfs
urlbase="https://anpuh.org.br/index.php/documentos/anais/category-items/1-anais-simposios-anpuh/5-snh04?start=260"
snh = '5-snh04'
pasta = '/home/ebn/Documentos/GitHub/Anais  Anpuh/{}/'.format(snh)
os.makedirs(pasta)
acabou = False

while acabou == False:
    response = request.urlopen(urlbase).read()
    soup= BeautifulSoup(response, "html.parser")     
    
    #find all paper boxes
    paperBoxes = soup.find_all(class_='has-context')
    print('Encontrando todos os papers da página...')
    for paper in paperBoxes:    
        title = paper.h2.text
        title = title.strip().lower().replace('/','-')
        # find all pdf links
        print('Encontrando link do paper...')
        link = paper.find('a', href=re.compile(r'(.pdf)'))
        fullLink = "https://anpuh.org.br" + link['href']
        fullName = pasta + title.replace(' ','_') + '.pdf'
        print('Salvando o pdf na pasta...')
        request.urlretrieve(fullLink, fullName)
    menuFinal = soup.find(class_='pagination')
    print('Procurando pŕoxima página...')
    proximaPag = menuFinal.find('a', title=re.compile(r'(Próx)'))
    linkProximaPag = proximaPag['href']
    print('https://anpuh.org.br' + linkProximaPag)
    
    try:
        urlbase = 'https://anpuh.org.br' + linkProximaPag
    except:
        acabou = True
        print("Final das Páginas de Papers desse evento.")
print('Final do código')
